<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Gauging the third dimension | Perception, Action &amp; Cognition - Course Handbook</title>
  <meta name="description" content="Your textbook guide to Perception, Action and Cognition" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Gauging the third dimension | Perception, Action &amp; Cognition - Course Handbook" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/Title.png" />
  <meta property="og:description" content="Your textbook guide to Perception, Action and Cognition" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Gauging the third dimension | Perception, Action &amp; Cognition - Course Handbook" />
  
  <meta name="twitter:description" content="Your textbook guide to Perception, Action and Cognition" />
  <meta name="twitter:image" content="images/Title.png" />

<meta name="author" content="Prof. Richard Wilkie, Dr Faisal Mushtaq, Dr Ryan Morehead and Prof. Mark Mon-Williams" />


<meta name="date" content="2020-09-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="vision-what-is-it-good-for.html"/>
<link rel="next" href="using-vision-to-control-the-hand.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Perception, Action, Cognition</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="why-do-we-percieve.html"><a href="why-do-we-percieve.html"><i class="fa fa-check"></i><b>2</b> Why do we percieve?</a><ul>
<li class="chapter" data-level="2.1" data-path="why-do-we-percieve.html"><a href="why-do-we-percieve.html#how-not-to-think-about-visual-perception"><i class="fa fa-check"></i><b>2.1</b> How NOT to think about Visual Perception</a></li>
<li class="chapter" data-level="2.2" data-path="why-do-we-percieve.html"><a href="why-do-we-percieve.html#active-perception"><i class="fa fa-check"></i><b>2.2</b> Active Perception</a></li>
<li class="chapter" data-level="2.3" data-path="why-do-we-percieve.html"><a href="why-do-we-percieve.html#why-is-perception-and-action-important"><i class="fa fa-check"></i><b>2.3</b> Why is Perception and Action Important?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html"><i class="fa fa-check"></i><b>3</b> Exploring the world using our eyes</a><ul>
<li class="chapter" data-level="3.1" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#why-do-we-move-our-eyes"><i class="fa fa-check"></i><b>3.1</b> Why do we move our eyes?</a></li>
<li class="chapter" data-level="3.2" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#different-types-of-eye-movements"><i class="fa fa-check"></i><b>3.2</b> Different Types of Eye-Movements</a><ul>
<li class="chapter" data-level="3.2.1" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#saccades"><i class="fa fa-check"></i><b>3.2.1</b> Saccades</a></li>
<li class="chapter" data-level="3.2.2" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#fixation"><i class="fa fa-check"></i><b>3.2.2</b> Fixation</a></li>
<li class="chapter" data-level="3.2.3" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#smooth-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Smooth Pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#vergence"><i class="fa fa-check"></i><b>3.2.4</b> Vergence</a></li>
<li class="chapter" data-level="3.2.5" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#vestibulo-ocular-reflex-vor"><i class="fa fa-check"></i><b>3.2.5</b> Vestibulo-Ocular Reflex (VOR)</a></li>
<li class="chapter" data-level="3.2.6" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#nystagmus"><i class="fa fa-check"></i><b>3.2.6</b> Nystagmus</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#how-do-we-control-eye-movements"><i class="fa fa-check"></i><b>3.3</b> How do we control eye-movements?</a></li>
<li class="chapter" data-level="3.4" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#what-do-eye-movements-allow-us-to-do"><i class="fa fa-check"></i><b>3.4</b> What do eye-movements allow us to do?</a></li>
<li class="chapter" data-level="3.5" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>3.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html"><i class="fa fa-check"></i><b>4</b> How do we drive?</a><ul>
<li class="chapter" data-level="4.1" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#the-core-components-of-driving"><i class="fa fa-check"></i><b>4.1</b> The Core Components of Driving</a></li>
<li class="chapter" data-level="4.2" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#what-information-is-available-for-controlling-steering"><i class="fa fa-check"></i><b>4.2</b> WHAT information is available for controlling steering?</a></li>
<li class="chapter" data-level="4.3" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#direct-vs.indirect-perception"><i class="fa fa-check"></i><b>4.3</b> Direct vs. Indirect Perception</a></li>
<li class="chapter" data-level="4.4" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#optic-flow-vs.visual-direction-information"><i class="fa fa-check"></i><b>4.4</b> Optic Flow vs. Visual Direction Information</a></li>
<li class="chapter" data-level="4.5" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#beyond-optic-flow"><i class="fa fa-check"></i><b>4.5</b> Beyond Optic Flow</a></li>
<li class="chapter" data-level="4.6" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#eye-movements-and-retinal-flow"><i class="fa fa-check"></i><b>4.6</b> Eye Movements and <em>Retinal Flow</em></a></li>
<li class="chapter" data-level="4.7" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#where-do-we-look-when-driving"><i class="fa fa-check"></i><b>4.7</b> Where do we look when driving?</a></li>
<li class="chapter" data-level="4.8" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#looking-at-the-tangent-point"><i class="fa fa-check"></i><b>4.8</b> Looking at the Tangent Point</a><ul>
<li class="chapter" data-level="4.8.1" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#formalising-the-relationship-between-fixating-the-tangent-point-and-steering"><i class="fa fa-check"></i><b>4.8.1</b> Formalising the relationship between fixating the Tangent Point AND steering</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#problems-for-the-tangent-point"><i class="fa fa-check"></i><b>4.9</b> Problems for the Tangent Point</a></li>
<li class="chapter" data-level="4.10" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#how-essential-is-the-tangent-point"><i class="fa fa-check"></i><b>4.10</b> How Essential is the Tangent Point?</a></li>
<li class="chapter" data-level="4.11" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#if-steering-is-shifted-what-happens-to-gaze"><i class="fa fa-check"></i><b>4.11</b> If Steering is Shifted, What Happens to Gaze?</a></li>
<li class="chapter" data-level="4.12" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#if-gaze-is-shifted-what-happens-to-steering"><i class="fa fa-check"></i><b>4.12</b> If Gaze is Shifted, What Happens to Steering?</a></li>
<li class="chapter" data-level="4.13" data-path="how-do-we-drive.html"><a href="how-do-we-drive.html#taking-the-racing-line"><i class="fa fa-check"></i><b>4.13</b> Taking the Racing Line</a></li>
<li class="chapter" data-level="4.14" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>4.14</b> References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html"><i class="fa fa-check"></i><b>5</b> Catching and Colliding</a><ul>
<li class="chapter" data-level="5.1" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#judging-time-to-contact-ttc"><i class="fa fa-check"></i><b>5.1</b> Judging Time to Contact (TTC)</a></li>
<li class="chapter" data-level="5.2" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#evidence-for-the-use-of-tau-in-the-real-world"><i class="fa fa-check"></i><b>5.2</b> Evidence for the Use of Tau in the Real-World</a></li>
<li class="chapter" data-level="5.3" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#is-tau-really-the-only-control-variable-for-interceptive-timing"><i class="fa fa-check"></i><b>5.3</b> Is Tau Really the Only Control Variable for Interceptive Timing?</a></li>
<li class="chapter" data-level="5.4" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#what-other-information-might-we-use-to-gauge-ttc"><i class="fa fa-check"></i><b>5.4</b> What Other Information Might We Use to Gauge TTC?</a></li>
<li class="chapter" data-level="5.5" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#virtual-reality-vr---to-go-in-a-blue-box"><i class="fa fa-check"></i><b>5.5</b> Virtual Reality (VR) - TO GO IN A BLUE BOX</a></li>
<li class="chapter" data-level="5.6" data-path="catching-and-colliding.html"><a href="catching-and-colliding.html#does-it-matter-where-you-look"><i class="fa fa-check"></i><b>5.6</b> Does it Matter Where You Look?</a></li>
<li class="chapter" data-level="5.7" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>5.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html"><i class="fa fa-check"></i><b>6</b> Cognition - What’s it all about?</a><ul>
<li class="chapter" data-level="6.1" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#so-what-is-cognitive-psychology"><i class="fa fa-check"></i><b>6.1</b> So, What is Cognitive Psychology?</a></li>
<li class="chapter" data-level="6.2" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#so-what-is-information"><i class="fa fa-check"></i><b>6.2</b> So, What is Information?</a></li>
<li class="chapter" data-level="6.3" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#cognition-action-interactions"><i class="fa fa-check"></i><b>6.3</b> Cognition Action Interactions</a></li>
<li class="chapter" data-level="6.4" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#so-what-is-perception"><i class="fa fa-check"></i><b>6.4</b> So, What is Perception?</a></li>
<li class="chapter" data-level="6.5" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#vision-is-a-robust-system"><i class="fa fa-check"></i><b>6.5</b> Vision is a Robust System</a></li>
<li class="chapter" data-level="6.6" data-path="cognition-whats-it-all-about.html"><a href="cognition-whats-it-all-about.html#visual-information"><i class="fa fa-check"></i><b>6.6</b> Visual information</a></li>
<li class="chapter" data-level="6.7" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>6.7</b> References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html"><i class="fa fa-check"></i><b>7</b> Vision - What is it good for?</a><ul>
<li class="chapter" data-level="7.1" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#processing-information"><i class="fa fa-check"></i><b>7.1</b> Processing information</a></li>
<li class="chapter" data-level="7.2" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#a-fundamental-problem"><i class="fa fa-check"></i><b>7.2</b> A fundamental problem</a></li>
<li class="chapter" data-level="7.3" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#the-nature-of-visual-processing"><i class="fa fa-check"></i><b>7.3</b> The nature of visual processing</a></li>
<li class="chapter" data-level="7.4" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#two-visual-streams"><i class="fa fa-check"></i><b>7.4</b> Two visual streams</a></li>
<li class="chapter" data-level="7.5" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#object-recognition"><i class="fa fa-check"></i><b>7.5</b> Object recognition</a></li>
<li class="chapter" data-level="7.6" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#object-recognition-physiology"><i class="fa fa-check"></i><b>7.6</b> Object recognition physiology</a></li>
<li class="chapter" data-level="7.7" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#object-recognition-neuropsychology"><i class="fa fa-check"></i><b>7.7</b> Object recognition neuropsychology</a></li>
<li class="chapter" data-level="7.8" data-path="vision-what-is-it-good-for.html"><a href="vision-what-is-it-good-for.html#categorising-faces"><i class="fa fa-check"></i><b>7.8</b> Categorising faces</a></li>
<li class="chapter" data-level="7.9" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>7.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html"><i class="fa fa-check"></i><b>8</b> Gauging the third dimension</a><ul>
<li class="chapter" data-level="8.1" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#distance-and-layout-perception"><i class="fa fa-check"></i><b>8.1</b> Distance and layout perception</a></li>
<li class="chapter" data-level="8.2" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#cue-combination"><i class="fa fa-check"></i><b>8.2</b> Cue combination</a></li>
<li class="chapter" data-level="8.3" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#distance-and-layout-information"><i class="fa fa-check"></i><b>8.3</b> Distance and layout information</a></li>
<li class="chapter" data-level="8.4" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#body-scaled-distance-cues"><i class="fa fa-check"></i><b>8.4</b> Body-scaled distance cues</a><ul>
<li class="chapter" data-level="8.4.1" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#accommodation"><i class="fa fa-check"></i><b>8.4.1</b> Accommodation</a></li>
<li class="chapter" data-level="8.4.2" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#vergence"><i class="fa fa-check"></i><b>8.4.2</b> Vergence</a></li>
<li class="chapter" data-level="8.4.3" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#vertical-image-disparities"><i class="fa fa-check"></i><b>8.4.3</b> Vertical image disparities</a></li>
<li class="chapter" data-level="8.4.4" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#vertical-gaze-angle"><i class="fa fa-check"></i><b>8.4.4</b> Vertical gaze angle</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#relative-information"><i class="fa fa-check"></i><b>8.5</b> Relative information</a><ul>
<li class="chapter" data-level="8.5.1" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#horizontal-retinal-image-disparities"><i class="fa fa-check"></i><b>8.5.1</b> Horizontal retinal image disparities</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#ordinal-information"><i class="fa fa-check"></i><b>8.6</b> Ordinal information</a></li>
<li class="chapter" data-level="8.7" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#how-is-vergence-used"><i class="fa fa-check"></i><b>8.7</b> How is vergence used?</a></li>
<li class="chapter" data-level="8.8" data-path="gauging-the-third-dimension.html"><a href="gauging-the-third-dimension.html#distance-perception-in-visual-form-agnosia"><i class="fa fa-check"></i><b>8.8</b> Distance perception in visual form agnosia</a></li>
<li class="chapter" data-level="8.9" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>8.9</b> References</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html"><i class="fa fa-check"></i><b>9</b> Using vision to control the hand</a><ul>
<li class="chapter" data-level="9.1" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#the-pre-contact-phase"><i class="fa fa-check"></i><b>9.1</b> The pre-contact phase</a></li>
<li class="chapter" data-level="9.2" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#the-spatial-path-of-the-transport-component"><i class="fa fa-check"></i><b>9.2</b> The spatial path of the transport component</a></li>
<li class="chapter" data-level="9.3" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#the-spatial-pattern-of-the-grasp-component"><i class="fa fa-check"></i><b>9.3</b> The spatial pattern of the grasp component</a></li>
<li class="chapter" data-level="9.4" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#the-timing-of-the-transport-component"><i class="fa fa-check"></i><b>9.4</b> The timing of the transport component</a></li>
<li class="chapter" data-level="9.5" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#what-does-mt-a-b-log2a-c-log2w-actually-mean"><i class="fa fa-check"></i><b>9.5</b> What does MT= a + b log2A + c log2W actually mean?</a></li>
<li class="chapter" data-level="9.6" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#the-contact-phase"><i class="fa fa-check"></i><b>9.6</b> The contact phase</a></li>
<li class="chapter" data-level="9.7" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#programming-fingertip-forces"><i class="fa fa-check"></i><b>9.7</b> Programming fingertip forces</a></li>
<li class="chapter" data-level="9.8" data-path="using-vision-to-control-the-hand.html"><a href="using-vision-to-control-the-hand.html#internal-models"><i class="fa fa-check"></i><b>9.8</b> Internal models</a></li>
<li class="chapter" data-level="9.9" data-path="exploring-the-world-using-our-eyes.html"><a href="exploring-the-world-using-our-eyes.html#references"><i class="fa fa-check"></i><b>9.9</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="mailto:f.mushtaq@leeds.ac.uk" target="blank">Spot an error? Email here</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Perception, Action &amp; Cognition - Course Handbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="gauging-the-third-dimension" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Gauging the third dimension</h1>
<p>Human vision begins with the images of objects on the retinae, but knowledge of an image’s retinal position is not sufficient for determining the location of a particular object with respect to a hand or the body.
Object localisation requires knowledge of ocular position with respect to the head, and head orientation with respect to the shoulders.
The important point that you should grasp at this stage is that <em>it is necessary to know the direction of the eyes in the head to gauge egocentric direction (egocentric direction means the direction relative to yourself)</em>.
Nevertheless, we are not going to deal with the topic of how the nervous system senses ocular position for the purpose of gauging direction.
Instead, we will consider egocentric distance perception (gauging the distance of an object from your body).
In order to interact skilfully with objects in the environment (e.g. reaching-to-grasp a cup) it is necessary to perceive the ‘third dimension’ but…</p>
<p><strong>The fundamental problem of gauging distance</strong> is that the image at the eye has two dimensions and thus has countless interpretations in three dimensions.</p>
<p>The issue of distance perception is closely related to the topic of ‘layout’ perception.
The perception of layout refers to how we see and understand the layout of objects in the world around us.
The perception of layout is often referred to as ‘depth’ perception but ‘layout’ is a more precise term.
If we consider the perception of layout we hit another fundamental difficulty:</p>
<p><strong>The fundamental problem of gauging layout</strong> is that the image at the eye has two dimensions and thus has countless interpretations in three dimensions.</p>
<p>It is important to distinguish between distance perception and the perception of layout.
I’ve already indicated that the two processes are highly related but this does not mean that they are equivalent.
In my opinion, it is possible to have distance perception in the absence of any perception of layout (gauging the distance of a single object in the dark) but impossible to have a perception of layout in the absence of distance perception.
I like to think that distance perception is concerned with gauging the distance of the point of fixation (where my eyes are pointing) and providing an anchor point to which the position of all other objects can be related.
Notably, the position of all objects away from my point of fixation can provide me with information regarding fixation distance whilst some information regarding layout requires knowledge of fixation distance in order that it can be interpreted.
It should also be noted that I am likely to continually change my point of fixation (see <strong>Lecture 2</strong> about eye movements).
These factors mean that distance and layout perception must surely interact with one another and be updated continually.
Indeed, Mon-Williams and Tresilian (1999) have shown that complex interactions exist between distance and layout perception.</p>
<p>In this lecture, we are going to consider primarily how the human nervous system gauges the egocentric distance of an object that is being fixated (i.e. fixation distance).
We will also make life easier for ourselves by only considering the case where the observer and the object are stationary.
This is clearly a massive simplification and one that ignores the reality of everyday life.
In fact, movement of the eyes and head provides us with an incredibly rich source of information regarding distance and layou t- information the human nervous system exploits.
Nevertheless, life (and this course) is short so we need to try and limit our consideration.</p>
<div id="distance-and-layout-perception" class="section level2">
<h2><span class="header-section-number">8.1</span> Distance and layout perception</h2>
<p>Cutting (1997) has provided a division of egocentric space into three regions:
<strong>Insert Figure here</strong>
This is a bit arbitrary but it is a useful starting point for our consideration.
The important point I want to get across is that personal space has been established as being approximately Euclidean in nature (not squashed up), with action and vista space having an affine character (squashed up)— although increasing the amount of available information decreases the affine nature of action space (Cutting 1997).
This means that the nervous system is able to gauge accurately the distance (and layout) of objects within personal space.
This observation raises the question of which sources of information (cues) can provide the distance information required for the Euclidean representation of personal space?</p>
<p>The information used for gauging target distance has attracted scientific interest for well over a century but Cutting (1997, p. 69) has pointed out that the result of this interest is “little more than a plenum of lists”.
Cutting is referring here to lists of potential distance cues, including; accommodation, aerial perspective, retinal image disparities, vergence angle, height in the visual field and so on.
For a long time, the author of every introductory textbook within psychology feels obliged to mindlessly list these cues (see Carlson for an example).
It is only recently that the issue of how the human nervous system uses these various sources of information has been addressed.</p>
<p>We will begin by considering the potential cues to distance.
The figure below provides an overview of the distance cues possibly used by the nervous system (it should be noted that we don’t fully understand distance perception so some of these cues may not be actually used or there may be sources that have yet to be identified).
There are other cues (such as shadowing) that contribute largely to the perception of object shape but whose role in distance perception is less clear.</p>
<div class="figure">
<img src="images/L8_F1_cropped.jpg" alt="Monocular and Binocular visual cues." />
<p class="caption">Monocular and Binocular visual cues.</p>
</div>
<p>In the figure, I have attempted to provide a basic taxonomy - the information is either binocular (only present when both eyes are viewing) or monocular (present with only one eye viewing).
I’ve further divided the information into that available from the retinae (retinal information) and that which arises from the ocular musculature (muscular).</p>
</div>
<div id="cue-combination" class="section level2">
<h2><span class="header-section-number">8.2</span> Cue combination</h2>
<p>The first thing we realise when we look at the figure above is that the system must somehow combine all of these cues.</p>
<p>It is extremely likely that information coming into the nervous system will contain ‘wrong’ bits of information — erroneous data if you like.
It seems reasonable to assume that the nervous system has evolved to deal with this problem— in other words the system is <em>robust</em>.
The term ‘robust’ used in this context originates in statistics where data can either be estimated in a non-robust fashion (e.g. the mean is a non-robust statistic) or a robust manner (the median is a robust statistic).
A robust statistic is simply one that optimally describes the data when outlying values are present.</p>
<p>A useful framework for considering cue combination is the <em>modified weak fusion</em> (MWF) process proposed by Landy et al. (1995).
In the MWF process one cue can promote another to allow it to provide information to the system but no other interactions are allowed.
The modified weak fusion account is consistent with existing data on cue combination and provides a parsimonious summary of extant empirical studies.</p>
<p>Modified weak fusion provides a good way of thinking about how the system combines different sources of information.
A crucial aspect of modified weak fusion (MWF) is that it allows one cue to promote another in order to allow it to provide information to the system (but no other interactions are allowed).</p>
</div>
<div id="distance-and-layout-information" class="section level2">
<h2><span class="header-section-number">8.3</span> Distance and layout information</h2>
<p>The reason that information needs to be ‘promoted’ is because not all information is immediately useful to the system.
In order to illustrate this point we need to consider the different types of information that are available.
The type of the information that arises can be described as <strong>body-scaled</strong>, <strong>relative</strong> or <strong>ordinal</strong>.
We’ll consider these in turn:</p>
</div>
<div id="body-scaled-distance-cues" class="section level2">
<h2><span class="header-section-number">8.4</span> Body-scaled distance cues</h2>
<p>Body-scale information provides a direct indication of the distance of surfaces from the body.
In other words, this is the gold standard of 3-D information that allows me to gauge directly egocentric distance.
Such information is scaled in relation to the perceptual systems of the observer.
This information category is occasionally referred to as ‘absolute’ but <strong>body-scaled</strong> best describes this information type.
Body scaled information is in principle available from the extra-retinal cues of <em>accommodation</em>, <em>vergence angle</em> and <em>vertical gaze angle</em> and the retinal information provided by <em>vertical disparities</em>.
Note that perceptual information is used to control the motoric accommodation (blur driven) and vergence (double images) systems but that these systems contribute, in turn, to perception (illustrating the importance of considering perception and action as two sides of the same coin).</p>
<div id="accommodation" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Accommodation</h3>
<p>It has long been suggested that accommodation might be a source of distance information (e.g. Berkeley 1732) and it has been shown that accommodation is a major distance cue for some animals (e.g. the Chameleon).
On the other hand, a considerable amount of work investigating whether accommodation information plays a role in human depth perception is generally in agreement that accommodation plays little or no role.
Fisher and Ciuffreda (1988) challenged this view and the pendulum of opinion fell towards accommodation playing a role in human distance perception (until we spoiled the party).
Mon-Williams &amp; Tresilian (2000) concluded that accommodation can act as a source of ordinal depth information in the absence of other depth cues, but questioned the contribution of accommodation to normal depth perception in full-cue conditions.
They suggested that accommodation’s ordinal role might be related to the use of vergence angle caused by the neural cross-linking of accommodation and vergence.</p>
</div>
<div id="vergence" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Vergence</h3>
<p>It has long been suggested that vergence might be a source of distance information (e.g. Berkeley 1709).
If we ‘know’ the distance between our eyes then simple triangulation can provide us with fixation distance if we are sensitive to vergence angle.
Nevertheless, there was some doubt about the usefulness of vergence derived distance information in normal (full cue) viewing conditions.
These doubts were founded on studies that used verbal reports of target distance to objects that were frequently placed at distances greater than 3m.
It is unlikely that vergence can provide useful information at such large distances (see later) and verbal reports are very unreliable.
James Tresilian and Mark Mon-Williams re-explored the role of vergence in distance perception and found evidence that vergence does contribute to distance perception.</p>
</div>
<div id="vertical-image-disparities" class="section level3">
<h3><span class="header-section-number">8.4.3</span> Vertical image disparities</h3>
<p>Humans (and a number of other species) have two eyes that are laterally separated in the head (by about 6cm in humans).
The fact that the eyes are separated means that each eye has a slightly different vantage point (i.e. the images at the back of the two eyes are different).
The retinal image differences are described as retinal image disparities (disparity just means difference) and these disparities can provide useful information to the nervous system.
There are actually two ways in which the retinal images are different - there are vertical differences and horizontal differences.
The vertical retinal image disparities provide direct body-scaled information and there is good evidence that the human nervous system uses this information in distance perception.</p>
</div>
<div id="vertical-gaze-angle" class="section level3">
<h3><span class="header-section-number">8.4.4</span> Vertical gaze angle</h3>
<p>Vertical gaze angle describes the angle (measured from the imaginary line between the eye and the ‘implicit horizon’) created when viewing a target located on a planar supporting surface.
If the height of the eye above a planar supporting surface orthogonal to gravity is known then vertical gaze angle provides a potentially accurate cue to a target’s egocentric distance.
There is good evidence that human observers use this cue (Gardner &amp; Mon-Williams, 2001; Mon-Williams et al. 2001).</p>
</div>
</div>
<div id="relative-information" class="section level2">
<h2><span class="header-section-number">8.5</span> Relative information</h2>
<p>Relative information allows an observer to gauge the relative distance that separates one object from another (i.e. object X is 20% closer than object Y).
Monocular retinal image cues provide relative information and are often described as pictorial cues.
The term <em>pictorial</em> thus refers to information sources such as height in scene, linear perspective and aerial perspective.
Artists have a good understanding of how these different cues can be used within visual art.
Unfortunately, our understanding of how the nervous system uses this information is pretty sparse.</p>
<p>It is my opinion that the system always seeks to produce a body-scaled representation of its three dimensional environment.
Thus, I consider it crucial to consider how the system ‘promotes’ (using the language of MWF) relative information in order that a body-scaled representation of distance and layout be achieved.
Generally, the processes of cue promotion are poorly understood but there is one exception - the promotion of horizontal retinal image disparities.</p>
<div id="horizontal-retinal-image-disparities" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Horizontal retinal image disparities</h3>
<p>We’ve already considered vertical retinal image disparities.
Horizontal retinal image disparities are different to their vertical counterparts because they do not provide body scaled information (i.e. they do not tell me about the distance of the object that I am fixating).
On the other hand, horizontal disparities do provide relative information regarding layout.
I suspect that almost all of you will have seen ‘magic eye pictures’ where you need to make vergence movements in order to see the ‘hidden 3-D picture’.
You may well have experienced other 3-D pictures where you peered down a viewing device or wore special glasses to see a 3-D image (not all of you will have seen the resulting three-dimensional pictures as around 5% of the population do not have the high level of binocular vision necessary).
These devices all work by ensuring that each eye receives an image that has horizontal differences.
Horizontal retinal image disparities provide a powerful impression of layout but they need to be ‘promoted’ in order to provide body-scaled information.
In order to promote the information it is necessary to know fixation distance.
It appears that the system uses a combination of vertical disparity and vergence information in order to gauge fixation distance for the purpose of promoting retinal image disparities.
In turn, it appears that horizontal retinal image information regarding layout can influence estimation of fixation distance (illustrating the complex interactions that exist between distance and layout perception).</p>
</div>
</div>
<div id="ordinal-information" class="section level2">
<h2><span class="header-section-number">8.6</span> Ordinal information</h2>
<p>Ordinal cues provide information regarding the order of objects.
The primary source of ordinal information is <em>occlusion</em>.
An object that occludes another may be assumed to be in front of the surface it masks.
Although this information may suggest the order of objects in the environment, it provides no impression of the relative or egocentric position of fixated objects.
Nevertheless, ordinal sources of information are likely to play an important role in promoting other information (e.g. through cue disambiguation).</p>
</div>
<div id="how-is-vergence-used" class="section level2">
<h2><span class="header-section-number">8.7</span> How is vergence used?</h2>
<p>If we really want to make progress in understanding distance perception we have to start tackling fundamental questions related to how the system uses the available information.
I have been emphasising that a good framework for considering cue combination is the <em>modified weak fusion</em> (MWF) process proposed by Landy et al. (1995).
Landy et al. developed a definition for an empirical measure of cue weight based on the weighted averaging model of cue combination.
The measure relies on perturbing one cue whilst holding several others constant.</p>
<p>Tresilian and Mon-Williams (Tresilian, Mon-Williams &amp; Kelly 1999; Tresilian &amp; Mon-Williams 1999) decided to adopt the MWF framework in order to explore how the human nervous system uses vergence information in distance perception.
They explored this issue by asking participants to point the end of an unseen stick at a visually specified target whilst they used ophthalmic prisms to manipulate vergence angle.
They investigated three factors that have been suggested to contribute to the determination of a cue’s weight:</p>
<ol style="list-style-type: lower-roman">
<li><p>the cue’s intrinsic reliability which is related to such factors as its signal to noise ratio.
The geometry of binocular vision indicates that the usefulness of vergence as a source of distance information is restricted to personal space — it is unlikely to provide any useful information for fixation distances greater than about 1.5m.
Since vergence angle gets smaller with increasing fixation distance, the reliability of the vergence cue decreases with fixation distance.
This means that increasing the fixation distance will decrease the signal-to-noise ratio of the vergence cue and, according to MWF, should result in a decreased weighting being attached to the vergence cue;</p></li>
<li><p>the degree to which the cue conflicts with information provided by the other available cues — the cue’s discrepancy;</p></li>
<li><p>the extent to which the weighting attached to the cue decreases as other information becomes available.</p></li>
</ol>
<p>The results suggested that:</p>
<ol style="list-style-type: lower-roman">
<li><p>vergence weight decreases with increasing fixation distance.</p></li>
<li><p>vergence plays a smaller role in distance perception if it is discrepant with other information about an object’s distance.</p></li>
<li><p>vergence weighting decreases as other cues become available.</p></li>
</ol>
<p>It is important to realise the implications of (iii).
The results showed that adding pictorial information altered the participant’s pointing behaviour (i.e. vergence and pictorial cues combined to guide action).</p>
</div>
<div id="distance-perception-in-visual-form-agnosia" class="section level2">
<h2><span class="header-section-number">8.8</span> Distance perception in visual form agnosia</h2>
<p>Mon-Williams, Tresilian, McIntosh and Milner (2001) investigated the cues used for the representation of personal space in a patient with visual form agnosia (DF).
Their results indicated that DF relies predominantly on binocular vergence information when determining the distance of a target despite the presence of other (retinal) cues.
Notably, DF is able to construct a Euclidean representation of personal space from vergence alone.
This finding supports the idea that vergence provides the nervous system with veridical information for the construction of personal space.
DF is also able to use horizontal image disparities in the perception of layout.</p>
<p>The results from this study suggest that the ventral stream is responsible for extracting layout information from ‘monocular’ retinal cues (on the basis that DF does not seem to be able to access this information).
The fact that DF can use horizontal image disparities suggests that this information is carried in the dorsal stream.
If it is the case that the ventral stream processes the pictorial cues and this information alters action (see previous section) then the implication is that the ventral stream does play a role in the control of action.
This would suggest that the ventral and dorsal streams are not completely independent in neurologically intact humans (i.e. the two streams interact).
It must be the case that the processing that occurs within the dorsal stream relates to that information most critical for supporting action.
Likewise, the ventral stream must process that information most critical for tasks such as object recognition.
Nevertheless, these deliberations suggest that the notion of two separate streams works best as a neuropsychological model for predicting patterns of behaviour in conditions such as visual form agnosia and optic ataxia.</p>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">8.9</span> References</h2>
<p>Berkeley, G. (1732). <em>An Essay Towards a New Theory of Vision</em> (4th ed.). First edition published 1709.</p>
<p>Cutting, J. E. (1997). <em>How the eye measures reality and virtual reality</em>. Behavior Research Methods, Instrumentation, &amp; Computers, 29, 29-36.</p>
<p>Fisher, S. K. &amp; Ciuffreda, K. J. (1988). <em>Accommodation and apparent distance</em>. Perception, 17, 609–621.</p>
<p>Gardner, P., Mon-Williams, M. (2001). <em>Vertical gaze angle: Absolute height-in-scene information for the programming of prehension</em>. Experimental Brain Research, 136, 379-385.</p>
<p>Landy, M. S., Maloney, L. T., Johnston, E. B. &amp; Young, M. (1995). <em>Measurement and modeling of depth cue combination: in defense of weak fusion</em>. Vision Research, 35, 389–412.</p>
<p>Mon-Williams, M., McIntosh, R. D., Milner, D. (2001). <em>Vertical gaze as a distance cue for programming reaching: Insights from visual form agnosia II (of III)</em>. Experimental Brain Research, 139, 137-142.</p>
<p>Mon-Williams, M., Tresilian, J. R. (1999). <em>A review of some recent studies on the extra-retinal contribution to distance perception</em>. Perception, 28, 167-181.</p>
<p>Mon-Williams, M., Tresilian, J. R. (2000). <em>An ordinal role for accommodation in distance perception</em>. Ergonomics, 43, 391-404.</p>
<p>– Mon-Williams, M., Tresilian, J. R., McIntosh, R. D., Milner, D. (2001). <em>Monocular and binocular distance cues: Insights from visual form agnosia I (of III)</em>. Experimental Brain Research, 139, 127-136.</p>
<p>– Tresilian, J. R., Mon-Williams, M. (2000). <em>Getting the measure of vergence weight in nearness perception</em>. Experimental Brain Research, 132, 362-368.</p>
<p>– Tresilian, J. R., Mon-Williams, M., Kelly, B. M. (1999). <em>Increasing confidence in vergence as a distance cue</em>. Proceedings of the Royal Society B266, 39-44.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="vision-what-is-it-good-for.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="using-vision-to-control-the-hand.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/07-gauging-the-third-dimension.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
